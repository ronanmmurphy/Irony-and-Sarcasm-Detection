{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zsckf2ldV6Cp"
   },
   "source": [
    "**Ronan Murphy**\n",
    "\n",
    "29/02/2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qndnYAQpL6I8"
   },
   "source": [
    "Developed a system to detect irony in text. Used data.txt file to train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vEvckziyL6I9"
   },
   "source": [
    "```csv\n",
    "Tweet index     Label   Tweet text\n",
    "1       1       Sweet United Nations video. Just in time for Christmas. #imagine #NoReligion  http://t.co/fej2v3OUBR\n",
    "2       1       @mrdahl87 We are rumored to have talked to Erv's agent... and the Angels asked about Ed Escobar... that's hardly nothing    ;)\n",
    "3       1       Hey there! Nice to see you Minnesota/ND Winter Weather \n",
    "4       0       3 episodes left I'm dying over here\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oc7hUIIjL6I9"
   },
   "source": [
    "# Part 1\n",
    "\n",
    "Read all the data and find the size of vocabulary of the dataset (ignoring case) and the number of positive and negative examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y-UeB-ShPiND"
   },
   "source": [
    "**Part 1: Description**\n",
    "\n",
    "- Imported text file with google colab\n",
    "\n",
    "- Seperated the rows by tab, converted tweets to a list and lowercased all text and added results to a set, got the length of this to determine vocabulary size of dataset = 17055\n",
    "\n",
    "- Grouped the data by label to get count for Ironic (1901) and non Ironic (1916)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "#use pandas to read the text file seperated by tab\n",
    "dataset = pd.read_csv('data.txt', sep = '\\t')\n",
    "\n",
    "\n",
    "#convert tweet text to list\n",
    "results = []\n",
    "tweet_data = dataset['Tweet text'].tolist()\n",
    "\n",
    "#for each tweet lowercase to ignore case and split into words\n",
    "for tweet in tweet_data:\n",
    "    tweet = tweet.lower()\n",
    "    results.extend([tweet_data for tweet_data in tweet.split()])\n",
    "#convert to set to return unique words\n",
    "results = set(results)\n",
    "print(results)\n",
    "#get size of vocabulary of dataset\n",
    "print(len(results))\n",
    "\n",
    "\n",
    "#number of positive and negative examples\n",
    "pos_neg_size = dataset.groupby('Label').size()\n",
    "print(pos_neg_size)\n",
    "\n",
    "#print first 10 rows\n",
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w5LivWxOL6JA"
   },
   "source": [
    "# Part 2\n",
    "\n",
    "Divide the data into a training and test set.\n",
    "\n",
    "Implement a function that calculates the precision, recall and F-Measure for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SSb_mFjaQjam"
   },
   "source": [
    "**Part 2: Description**\n",
    "\n",
    "- Import Stopwords from NLTK to use for feature extraction\n",
    "\n",
    "- Split the data into X (Tweet text) and Y (Labels) values, after feature extraction will split into training an test with a ratio 80-20 as this avoids over training but returns the best model accuracy.\n",
    "\n",
    "- Created a method 'Scores' to return the Accuracy, Precsion, Recall and F1-Score of the predicted Y labels compared to the acutual labels. This can be called for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "# import nltk to get stopwords package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#add tweets and labels to X and Y respectively \n",
    "x= dataset.iloc[:,2].values\n",
    "y = dataset.iloc[:,1].values\n",
    "\n",
    "\n",
    "\n",
    "#method to get accuracy, precision recall and f-score \n",
    "def scores(y_test, y_predicted):\n",
    "  # accuracy: (tp + tn) / (p + n)\n",
    "  accuracy = accuracy_score(y_test, y_predicted)\n",
    "  print('Accuracy: %f' % accuracy)\n",
    "  # precision tp / (tp + fp)\n",
    "  precision = precision_score(y_test, y_predicted)\n",
    "  print('Precision: %f' % precision)\n",
    "  # recall: tp / (tp + fn)\n",
    "  recall = recall_score(y_test, y_predicted)\n",
    "  print('Recall: %f' % recall)\n",
    "  # f1: 2 tp / (2 tp + fp + fn)\n",
    "  f1 = f1_score(y_test, y_predicted)\n",
    "  print('F1 score: %f' % f1)\n",
    "  return precision, recall, f1\n",
    "\n",
    "\n",
    "\n",
    "#implement split in next task when extract features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nU9HGp4eo_0C"
   },
   "source": [
    "# Part 3\n",
    "\n",
    "Extracted features from each sentence. Implemented a simple log-linear model to classify tweets as ironic or not ironic.\n",
    "\n",
    "Trained this method and evaluate the results using precision, recall and F-Measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OMjSV7tyRmCg"
   },
   "source": [
    "**Part 3: Description**\n",
    "\n",
    "- First extract the stopwords features using NLTK as they add no extra sentiment to the tweet text. This can be used to improve accuracy of a model as it removes redundant information that can skew results. In part 5 I use more feature extraction as it can be seen that usernames, web addresses etc dont add more sentiment either\n",
    "\n",
    "- Using count vectoriser covert the text to unique vectors, lowercasing all text so it can be analysed by a model\n",
    "\n",
    "- split the data into 80:20 train test split \n",
    "\n",
    "- create Logistic Regression model which uses the signmoid function (Log-linear)\n",
    "\n",
    "- Call the 'Scores' method to get the accuracy, precision, recall, f1-score :-\n",
    "\n",
    "Accuracy: 0.649215;\n",
    "Precision: 0.646597;\n",
    "Recall: 0.650000;\n",
    "F1 score: 0.648294\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "WPAFNvmRGNUM",
    "outputId": "9080c8e2-1714-4883-a92c-99a113d9a425"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3817\n",
      "(3053, 12725)\n",
      "(3053,)\n",
      "Accuracy: 0.649215\n",
      "Precision: 0.646597\n",
      "Recall: 0.650000\n",
      "F1 score: 0.648294\n",
      "(0.6465968586387435, 0.65, 0.6482939632545932)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "#remove stopwords features from dataset to improve accuracy\n",
    "stopWords = set(stopwords.words('english'))\n",
    "wordsFiltered = []\n",
    "for w in x:\n",
    "    if w not in stopWords:\n",
    "        wordsFiltered.append(w)\n",
    "vocab = set(wordsFiltered)\n",
    "print(len(vocab))\n",
    "#use count vectoriser to convert words to unique IDs and lowercase the words\n",
    "vectorizer = CountVectorizer(\n",
    "    analyzer = 'word',\n",
    "    lowercase = True,\n",
    "\n",
    ")\n",
    " \n",
    "features = vectorizer.fit_transform(\n",
    "    wordsFiltered\n",
    ")\n",
    "\n",
    "features_nd = features.toarray() \n",
    "\n",
    "#splitting with 80:20 as for this size of data this will train on 80% of data and test on the rest.\n",
    "#if train is too high then will overtrain and underfit if too low \n",
    "#this returns an accuracy of just under 65%\n",
    "x_train, x_test, y_train, y_test = train_test_split(features_nd, y, train_size=0.80, random_state=1234)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "#train logistic regression that uses sigmoid activaiton function with data\n",
    "log_model = LogisticRegression()\n",
    "log_model = log_model.fit(X=x_train, y=y_train)\n",
    "#predict y values from test\n",
    "y_pred = log_model.predict(x_test)\n",
    "\n",
    "#predict scores with method, returns accuracy, precsion, recall and fscore\n",
    "print(scores(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "82JnhmgBL6JA"
   },
   "source": [
    "# Part 4\n",
    "\n",
    "Developed an acceptor recurrent neural network that classifiers the sentence as ironic or not ironic.\n",
    "\n",
    "Evaluated this according to precision, recall or F-Measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X6hm8wyCUqAX"
   },
   "source": [
    "**Part 4: Description**\n",
    "\n",
    "- Created Acceptor RNN, Tokenised the words using keras preprocessing tools and added padding to each tweet to make them the same length. This is another feautre in part 5 which can be improved as one long tweet can skew the results of all others as they will add zeros to the max length. Truncating to and average length is a possible solution.\n",
    "\n",
    "- Split the data into train, test with 80-20. Inan improved model can use validate data also which is an unbiased evaluation of hyperparameters\n",
    "\n",
    "- Created 3 layered sequential model with emdedding, bidirectional LSTM and dense output layer that uses softmax to output results. Set the hyperparameters on these for the amount of nodes on each layer. Increasing nodes can improve accuracy but increase time to train \n",
    "\n",
    "- Compiled this model with binary crossentropy and a learning rate using adam to compare accuracy\n",
    "\n",
    "- fitted the model for 10 epochs and batch size of 64 to increase the speed it returns each epoch, Test Accuracy = 58.74%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.upload()\n",
    "#import file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "dataset = pd.read_csv('data.txt', sep = '\\t')\n",
    "\n",
    "#tokenize the dataset with max features set to 30,000, split by spaces\n",
    "t = Tokenizer(nb_words=30000, split=' ')\n",
    "#fit the text of tweets to tokenized vectors\n",
    "t.fit_on_texts(dataset['Tweet text'].values)\n",
    "#convert the text in tweets to tokenized values\n",
    "X1 = t.texts_to_sequences(dataset['Tweet text'].values)\n",
    "#pad the Tweet text column with 0's. this adds zeros so all tweets are same size, this may skew results as one long tweet can add zeros for all tweets\n",
    "X1 = pad_sequences(X1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the labels for the X1 converted data \n",
    "Y1 = pd.get_dummies(dataset['Label']).values\n",
    "#split the data into train and test with 80%-20% train-test\n",
    "X1_train, X1_test, Y1_train, Y1_test = train_test_split(X1,Y1, train_size =0.8)\n",
    "#get the shape of the data sets\n",
    "print(X1_train.shape,Y1_train.shape)\n",
    "print(X1_test.shape,Y1_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "#create acceptor RNN model to classify tweets using tensorflow\n",
    "#3 layers the original embedding, LSTM layer and output layer\n",
    "model = tf.keras.Sequential([\n",
    "    #size of vocab and output dimensions\n",
    "    tf.keras.layers.Embedding(30000, 128, input_length=X1.shape[1]),\n",
    "    #LSTM with 128 nodes\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256)),\n",
    "    #dense output layer binary therefore only 1\n",
    "    tf.keras.layers.Dense(2, activation='softmax')\n",
    "])\n",
    "#compile as binary output with adam learning rate comparing accuracy\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=['accuracy'])\n",
    "#give summary of model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "#fit the model to training data \n",
    "history = model.fit(x=X1_train,y=Y1_train, batch_size=batch_size,\n",
    "                    epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare output with test data\n",
    "results = model.evaluate(x= X1_test, y=Y1_test)\n",
    "\n",
    "print('test loss, test acc:', results)\n",
    "#compare scores accuracy, f1 and recall and precision\n",
    "y_pred = model.predict(X1_test)\n",
    "\n",
    "yp= []\n",
    "for x in y_pred:\n",
    "  x= x.astype(int)\n",
    "  yp.append(x)\n",
    "\n",
    "yp = np.array(yp)\n",
    "\n",
    "print(scores(Y1_test, yp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9yZucGZmL6JM"
   },
   "source": [
    "# Part 5\n",
    "\n",
    "Enchanced the RNN developed in Part 4 and which shows an improvement according to your evaluation metric.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5D2f4LVtUQoo"
   },
   "source": [
    "**Improvement RNN**\n",
    "\n",
    "To improve the original modle decided to change the Acceptor RNN from using keras tokenizer to using my own method from scratch to convert unique numbers to ints so I could edit the padding sizes to improve the accuracy. \n",
    "\n",
    "I also added more extracting features to remove punctuation, usernames, web addresses and numbers as they might skew the results. By adding custom padding I got the max and avg length of words. This was found to be 149 and 12.5 respectively. Therefore there were many 0's in each vectorised tweet which added no meaning and truncating tweets over a size of 15 gave an improved accuracy. If tweets were shorter than this they would be paddded with 0's but this had less of an affect on the results.\n",
    "\n",
    "I also added two extra hidden layers to see if the model would improve results by training better. Although it gave slight improvements this didnt massively change the output accuracy.\n",
    "\n",
    "Finally I trained the model and evaluated with the scores method returning:-\n",
    "\n",
    "Accuracy: 0.67121;\n",
    "Precision: 0.65292;\n",
    "Recall: 0.66203;\n",
    "F1 score: 0.66119\n",
    "\n",
    "\n",
    "This proves that by extracting more featueres and adding more layers that it improved the model in terms of accuracy and predictability. To get further improvments adding more layers or editing the hyperparameters might give a better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv('data.txt', sep =\"\\t\")\n",
    "x= dataset.iloc[:,2].values\n",
    "y = dataset.iloc[:,1].values\n",
    "punctuation = '!\"$%&\\'()*+,-./:;<=>?[\\\\]^_`{|}~'\n",
    "# remove punctuation\n",
    "all_tweets = 'separator'.join(x)\n",
    "all_tweets = all_tweets.lower()\n",
    "all_text = ''.join([c for c in all_tweets if c not in punctuation])\n",
    "\n",
    "# split by new lines and spaces\n",
    "tweets_split = all_text.split('separator')\n",
    "all_text = ' '.join(tweets_split)\n",
    "\n",
    "# create a list of words\n",
    "words = all_text.split()\n",
    "\n",
    "# remove web address, twitter iusername, and numbers from tweet text\n",
    "new_reviews = []\n",
    "for review in tweets_split:\n",
    "    review = review.split()\n",
    "    new_text = []\n",
    "    for word in review:\n",
    "        if (word[0] != '@') & ('http' not in word) & (~word.isdigit()):\n",
    "            new_text.append(word)\n",
    "    new_reviews.append(new_text)\n",
    "\n",
    "print(new_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "S09Mp0j4KfBG",
    "outputId": "59cb9b61-1963-4d8a-80fb-2df117d5cc8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words:  14173\n",
      "Tokenized review: \n",
      " [[815, 2146, 1186, 418, 19, 7, 60, 11, 65, 3665, 3666]]\n",
      "1966\n",
      "y 3816\n",
      "x 3816\n",
      "[]\n",
      "1966\n",
      "12.60272536687631 mean\n",
      "149\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "## Build a dictionary that maps words to integers\n",
    "counts = Counter(words)\n",
    "vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "vocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)}\n",
    "\n",
    "## use the dict to tokenize each review in reviews_split\n",
    "## store the tokenized reviews in reviews_ints\n",
    "reviews_ints = []\n",
    "for review in new_reviews:\n",
    "    reviews_ints.append([vocab_to_int[word] for word in review])\n",
    "\n",
    "\n",
    "# stats about vocabulary\n",
    "print('Unique words: ', len((vocab_to_int)))  # should ~ 74000+\n",
    "\n",
    "\n",
    "# print tokens in first review\n",
    "print('Tokenized review: \\n', reviews_ints[:1])\n",
    "\n",
    "\n",
    "#find tweets of 0 length\n",
    "for i in reviews_ints:\n",
    "\n",
    "  \n",
    "  if len(i)==0:\n",
    "    print(i)\n",
    "    print(count)\n",
    "  count+=1\n",
    "  \n",
    "#method to remove the 1966 element which was empty after feature extraction\n",
    "list =[]\n",
    "count = 0\n",
    "for elements in reviews_ints:\n",
    "  if count != 1966:\n",
    "    list.append(elements)\n",
    "  else:\n",
    "    print(count)\n",
    "  count +=1\n",
    "\n",
    "#remove the same y value\n",
    "labels = np.delete(y, [1966])\n",
    "print(\"y\", np.size(labels))\n",
    "print(\"x\", len(list))\n",
    "count = 0\n",
    "\n",
    "\n",
    "#get mean and max of tweet length to use for padding\n",
    "mean_list = []\n",
    "for i in list:\n",
    "  mean_list.append(len(i))\n",
    "from statistics import mean\n",
    "print(mean(mean_list), \"mean\")\n",
    "\n",
    "\n",
    "maxList = max(list, key = len) \n",
    "maxLength = max(map(len, list)) \n",
    "print(maxLength)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "8s4cnKQSKRMS",
    "outputId": "6fb592a8-83e1-430e-e941-4c760d19b66e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0    0  815 2146 1186  418   19    7   60   11   65 3665\n",
      "  3666]\n",
      " [  37   25 3669    2   23 1566    2 3670 2147    6    1 3671  525   42\n",
      "  1567]\n",
      " [   0    0    0    0    0    0  325   83  140    2   66    9 3673  366\n",
      "   419]\n",
      " [   0    0    0    0    0    0    0    0    0 3674  367   26  640  100\n",
      "   121]\n",
      " [   4   58 1188   33 2148   47    1  127 2149 1568    8    1   91    7\n",
      "    46]\n",
      " [   0    0    0    0    0    0    0    0  118  114   89  282   11 3677\n",
      "  3678]\n",
      " [ 157  197   20 2151  104  135   12    1 1191    6  420  299  641  954\n",
      "    30]\n",
      " [   0    0    0    0   46 2152   10  160 1569   45  209   17    3  251\n",
      "  3681]\n",
      " [  41    9   79    9   76  368    3  237   61 3682   48   31 1192 3683\n",
      "   192]\n",
      " [   0    0    0    0    0    9   25   22 1194    2  526   13  252   65\n",
      "    38]]\n"
     ]
    }
   ],
   "source": [
    "#pad method to add zeros for length of padding set to 15, if tweet >15 length then truncate the tweet\n",
    "def pad_features(list, seq_length):\n",
    "    # getting the correct rows x cols shape\n",
    "    features = np.zeros((len(list), seq_length), dtype=int)\n",
    "\n",
    "    # for each review, I grab that review and \n",
    "    for i, row in enumerate(list):\n",
    "      features[i, -len(row):] = np.array(row)[:seq_length]\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Test implementation!\n",
    "\n",
    "seq_length = 15\n",
    "\n",
    "features = pad_features(list, seq_length=seq_length)\n",
    "\n",
    "\n",
    "\n",
    "# print first 10 values of the first 15 tweets \n",
    "print(features[:10,:150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "aj11RgZVKLu4",
    "outputId": "f9623260-3d47-442f-a19e-30904e6fdeb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(3052, 15) \n",
      "Validation set: \t(382, 15) \n",
      "Test set: \t\t(382, 15)\n",
      "Train set: \t\t(3052,) \n",
      "Validation set: \t(382,) \n",
      "Test set: \t\t(382,)\n"
     ]
    }
   ],
   "source": [
    "split_frac = 0.8 # best split ratio chosen from tests in part 4\n",
    "\n",
    "## split data into training, validation, and test data for x and y \n",
    "\n",
    "split_idx = int(len(features)*split_frac)\n",
    "train_x, remaining_x = features[:split_idx], features[split_idx:]\n",
    "train_y, remaining_y = labels[:split_idx], labels[split_idx:]\n",
    "\n",
    "test_idx = int(len(remaining_x)*0.5)\n",
    "val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\n",
    "val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "#create model similar to before with extra nodes \n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(14200, 7500),# let output dimensions equal to half input to give better results, input size reduced with extraction\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256)), # increase nodes to 256\n",
    "    tf.keras.layers.Dense(128, activation='relu'), # increase nodes to 128\n",
    "    tf.keras.layers.Dense(1) # output layer\n",
    "])\n",
    "#same means of compiling binary cross entropy adam learner\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x=train_x,y=train_y,\n",
    "                    batch_size=64,\n",
    "                    epochs=10,\n",
    "                    validation_data = (val_x, val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(x= test_x, y=test_y)\n",
    "\n",
    "print('test loss, test acc:', results)\n",
    "\n",
    "y_pred = model.predict(test_x)\n",
    "print(scores(test_y, y_pred))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Advanced_Topics_in_Natural_Language_Processing_Assignment_1_Ronan_Murphy_15397831.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
